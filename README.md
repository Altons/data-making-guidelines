# data-making-guidelines
documenting the DataMade ETL workflow

## Contents
- [DataMade's Data Making Principles](https://github.com/datamade/data-making-guidelines#datamades-data-making-principles)
- [Implementation Specifics - Makefiles](https://github.com/datamade/data-making-guidelines#implementation-specifics---makefiles)
  - [Introduction to make & makefiles](https://github.com/datamade/data-making-guidelines#introduction-to-make--makefiles)
  - [Makefile 101](https://github.com/datamade/data-making-guidelines#makefile-101)
  - [ETL workflow directory structure](https://github.com/datamade/data-making-guidelines#etl-workflow-directory-structure)


## DataMade's Data Making Principles

1. Never destroy data 
2. Ensure that each step in process is audit-able 
3. Be able to reproduce the final data with one command 
4. Write as little custom code as possible 
5. Use standard tools whenever possible 

## Implementation Specifics - Makefiles

To achieve a reproducible data workflow, we use GNU make

#### Introduction to make & makefiles
A simple way of thinking about a data processing workflow is as a series of steps. However, instead of thinking *forward*, in terms of an order of steps from step 1 to step N, you can also also think *backwards* - in terms of the outputs that you want and the files that those outputs are derived from. Thinking backwards is a more powerful way of expressing a data workflow, since dependencies aren't always linear.

```make``` is a build tool that generates file *targets*, each of which can depend upon the existence of other files (*dependencies*). Targets, dependencies, and instructions specifying to build them are defined in a *makefile*. The nice thing about makefiles is that once you specify a dependency graph, make will do the work of figuring out the individual steps required to build an output, based on your rules and the files you already have.

**```make``` is a particularly nifty tool for data processing because**:
- make allows you to create all final data with a single command, since ```make``` rules can be chained. writing a ```makefile``` is ultimately an exercise in making your existing data processing steps explicit, to ultimately avoid manual, undocumented steps
- ```make``` is smart about only building what's necessary, because it's aware of when a file was last modified - ```make``` will not rebuild existing files if their dependencies haven't changed.
- some more reasons


#### Makefile 101
When you run a ```make``` command, ```make``` will look for instructions in a file called ```Makefile``` in the current directory. The building block of a makefile is a "rule". Each "rule" specifies (1) a *target*, (2) the target's *dependencies*, and the target's *recipe* (i.e. the commands for creating the target).

The general structure of a single make "rule":
```
target: dependencies
[tab] recipe
```
**Targets** - the target is what you want to generate. it can be a the name of an output file, or a variable (more on this later)  
**Dependencies** - dependencies are optional. dependencies can be the names of files that need to exist in order to make the target, or variables (more on this later)  
**Recipes** - recipes are commands for generating the target file. any command you can run on the terminal is fair game  for recipes - bash commands, invoking a script, etc.  

[some content here about how make determines what to make & in what order, based on the rules & what files exist]

#### ETL workflow directory structure

[some notes on the directory structure, on top level makefile & sub makefiles]

```
|-- Makefile
|-- README.md
|-- data
|   |-- <group of files for processing>
|   |   |-- Makefile
|   |   |-- README.md # Gives details about the source of the data and about how
|   |   |             # it is processed by the pipeline
|   |   |-- build
|   |   |   `-- <temporary files generated by pipeline live here>
|   |   |-- finished_files
|   |   |   `-- <finished product of pipeline lives here>
|   |   |-- mid_process_files
|   |   |   `-- <mid-process files live here>
|   |   `-- raw
|   |       `-- <raw data files live here>
|   |-- <another group of files for processing>
|   |   `-- < ... etc ... >
|-- processors
|   |-- <processor_name>.py
|   `-- <another_processor_name>.sh
`-- requirements.txt # Lists requirements that need to be installed for pipeline
```

## Examples
- [Gary Counts](https://github.com/datamade/gary-counts-data)

## Related Links
- [Makefile Style Guide by Clark Grubb](http://clarkgrubb.com/makefile-style-guide#data-workflows)
- [Why Use Make by Mike Bostock](http://bost.ocks.org/mike/make/)
